//  Copyright (c) 2023 Advanced Micro Devices, Inc. All Rights Reserved.
//
//  Permission is hereby granted, free of charge, to any person obtaining a copy
//  of this software and associated documentation files (the "Software"), to deal
//  in the Software without restriction, including without limitation the rights
//  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
//  copies of the Software, and to permit persons to whom the Software is
//  furnished to do so, subject to the following conditions:
//
//  The above copyright notice and this permission notice shall be included in all
//  copies or substantial portions of the Software.
//
//  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
//  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
//  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
//  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
//  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
//  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
//  SOFTWARE.

#version 450
#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_shader_explicit_arithmetic_types_int16 : require

// Shaders must be built with SPIRV >= 1.3 as subgroups require it
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_basic : require

#include "shaderEnqueueSpirvIntrinsics.glsl.h"

#define TILE_SIZE 			16
#ifdef NODE_AGGREGATION
#define MAX_ALLOCATIONS 	256	// TILE_SIZE*TILE_SIZE
#else
#define MAX_ALLOCATIONS		1	// 1 payload per tile
#endif

layout(local_size_x = TILE_SIZE, local_size_y = TILE_SIZE) in;

struct OutputPayload
{
#ifdef NODE_DYNAMIC_EXPANSION
	uvec3   grid_size;
#endif
	u16vec2 coord;		// tile or pixel coordinates
};

DeclareMaxNumWorkgroupsAMDX(512, 512, 1);	// Up to 8k-by-8k resolution
DeclareOutputPayloadAMD("compose", InitializePayloads, FinalizePayloads, OutputPayload, out_payload, MAX_ALLOCATIONS);

// All shaders must use the same resource bindings (it's the same pipeline)

layout (set = 0, binding = 2) uniform usampler2D inMaterial;

// gl_NumSubgroups is not a constant, but we can allocate a large enough array.
// The subgroup will be at least 32 invocations.
shared uint sharedPerSubgroupMask[(TILE_SIZE*TILE_SIZE)/32];
shared uint sharedShaderIndex;

void main()
{
	const ivec2 coord = ivec2(TILE_SIZE * gl_WorkGroupID.xy + gl_LocalInvocationID.xy);
	const uint material = texelFetch(inMaterial, coord, 0).r;		// zero-based material index

#ifdef NODE_AGGREGATION
	const uint recordCount  = 1;		// one record per invocation
	const uint shaderIndex  = material;	// use the shader corresponding to that material ID

	InitializePayloads(out_payload, ScopeInvocation, recordCount, shaderIndex);

	out_payload[0].coord = u16vec2(coord);

	FinalizePayloads(out_payload);


#else // not NODE_AGGREGATION

	sharedPerSubgroupMask[gl_SubgroupID] = subgroupOr(material);

	barrier();

	if (gl_LocalInvocationIndex == 0)
	{
		sharedShaderIndex = 0;

		for (int i = 0; i < gl_NumSubgroups; ++i)
		{
			sharedShaderIndex = sharedShaderIndex | sharedPerSubgroupMask[i];
		}
		// sharedShaderIndex corresponds to the common mask of all materials in this tile
	}

	barrier();

	const uint recordCount     = 1;	// only one payload

	InitializePayloads(out_payload, ScopeWorkgroup, recordCount, sharedShaderIndex);

	if (gl_LocalInvocationIndex == 0)
	{
#ifdef NODE_DYNAMIC_EXPANSION
		out_payload[0].grid_size = uvec3(1, 1, 1);
#endif
		out_payload[0].coord = u16vec2(gl_WorkGroupID);	// z is discarded
	}

	FinalizePayloads(out_payload);

#endif // not NODE_AGGREGATION
}
